{"cells":[{"cell_type":"markdown","source":["### **Northwind** (Silver Transformation)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db12f70f-643b-4870-98de-df966a14470a"},{"cell_type":"code","source":["# Import Requirements\n","import json\n","import re\n","from notebookutils import mssparkutils\n","from pyspark.sql import SparkSession, DataFrame, functions as F, types as T\n","from delta.tables import DeltaTable\n","from typing import Sequence, Optional\n","from pyspark.sql import DataFrame"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e13babbf-b4f9-453a-9ea6-e7a864ca07a4","normalized_state":"finished","queued_time":"2026-01-06T08:41:27.4587795Z","session_start_time":"2026-01-06T08:41:27.4598977Z","execution_start_time":"2026-01-06T08:41:40.7476122Z","execution_finish_time":"2026-01-06T08:41:41.0330194Z","parent_msg_id":"35ef8657-7188-46dc-b01e-775f18ec0ce3"},"text/plain":"StatementMeta(, e13babbf-b4f9-453a-9ea6-e7a864ca07a4, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a886d43a-a7d7-4497-b843-059787a2e5fc"},{"cell_type":"markdown","source":["#### **Get Bronze Metadata**\n","Receives Bronze Table Metadata from Copy Activity via _Data Factory Pipeline_"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"705432f0-9a45-49f8-bdb9-c6b56b924f39"},{"cell_type":"code","source":["# Paramameterized Value\n","# bronze_metadata = \"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"c60f3891-3d52-47e3-bfea-06ecbbdf6fa6","normalized_state":"finished","queued_time":"2026-01-05T21:34:02.914332Z","session_start_time":null,"execution_start_time":"2026-01-05T21:34:02.9156809Z","execution_finish_time":"2026-01-05T21:34:03.2117048Z","parent_msg_id":"3818696a-dc21-42f6-946b-cf3e8ead6ade"},"text/plain":"StatementMeta(, c60f3891-3d52-47e3-bfea-06ecbbdf6fa6, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"d6384cb8-3cbb-469f-8f71-94f4ac94b016"},{"cell_type":"code","source":["def clean_json(payload: str) -> list:\n","    \"\"\"Parse & un-escape incoming JSON array\"\"\"\n","    raw = payload.strip()\n","    if raw.startswith('\"') and raw.endswith('\"'):\n","        raw = raw[1:-1].replace('\\\\\"', '\"') #Replaces the \\\" with \"\n","    records = json.loads(raw)\n","    return records\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"c60f3891-3d52-47e3-bfea-06ecbbdf6fa6","normalized_state":"finished","queued_time":"2026-01-05T21:34:11.0307781Z","session_start_time":null,"execution_start_time":"2026-01-05T21:34:11.0319916Z","execution_finish_time":"2026-01-05T21:34:11.2822976Z","parent_msg_id":"e0744390-ae20-4423-8929-2792be88d803"},"text/plain":"StatementMeta(, c60f3891-3d52-47e3-bfea-06ecbbdf6fa6, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b36c0cdc-6720-43aa-af90-6eec8e1f80df"},{"cell_type":"code","source":["# STEP 1: Parse & Normalize Incoming Metadata\n","parsed = clean_json(bronze_metadata)\n","\n","# STEP 2: Unwrap nested BronzeMetadata\n","records = [entry[\"BronzeMetadata\"] for entry in parsed if \"BronzeMetadata\" in entry]\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fade98ab-52c2-42f7-83b0-181c084464e3"},{"cell_type":"code","source":["#In case debugging is needed \n","# print(parsed)\n","# print('//////////////////////////////////////////////////////////')\n","# print(records)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"c60f3891-3d52-47e3-bfea-06ecbbdf6fa6","normalized_state":"finished","queued_time":"2026-01-05T21:39:29.4333055Z","session_start_time":null,"execution_start_time":"2026-01-05T21:39:29.434358Z","execution_finish_time":"2026-01-05T21:39:29.7128758Z","parent_msg_id":"195bb66a-705d-4474-82e7-2ad3116d36ac"},"text/plain":"StatementMeta(, c60f3891-3d52-47e3-bfea-06ecbbdf6fa6, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb3ff3e6-64b9-4e73-b091-2e498c3cf6e5"},{"cell_type":"markdown","source":["#### **Data Transformation**\n","Custom Cleaning Functions for Each Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa483f69-d4c9-420a-90a7-c5bb82a28d16"},{"cell_type":"code","source":["def clean_categories(raw_df):\n","    # Capitalize the first character in column: 'Description'\n","    raw_df = raw_df.withColumn('Description', F.initcap(F.col('Description')))\n","    # Drop column: 'Picture'\n","    raw_df = raw_df.drop('Picture')\n","    return raw_df\n","\n","def clean_customers(raw_df):\n","    # Capitalize the first character in column: 'CompanyName'\n","    raw_df = raw_df.withColumn('CompanyName', F.initcap(F.col('CompanyName')))\n","    # Capitalize the first character in column: 'Address'\n","    raw_df = raw_df.withColumn('Address', F.initcap(F.col('Address')))\n","    return raw_df\n","\n","def clean_employees(raw_df):\n","    # Change column type to datetime64[ns] for column: 'BirthDate'\n","    raw_df = raw_df.withColumn('BirthDate', raw_df['BirthDate'].cast(T.TimestampType()))\n","    # Change column type to datetime64[ns] for column: 'HireDate'\n","    raw_df = raw_df.withColumn('HireDate', raw_df['HireDate'].cast(T.TimestampType()))\n","    # Rename column 'ReportsTo' to 'Manager'\n","    raw_df = raw_df.withColumnRenamed('ReportsTo', 'Manager')\n","    # Drop columns: 'PhotoPath', 'Photo'\n","    raw_df = raw_df.drop('PhotoPath', 'Photo')\n","    return raw_df\n","\n","def clean_employee_territories(raw_df):\n","    # Change column type to int64 for column: 'TerritoryID'\n","    raw_df = raw_df.withColumn('TerritoryID', raw_df['TerritoryID'].cast(T.LongType()))\n","    return raw_df\n","\n","def clean_order_details(raw_df):\n","    # Change column type to float64 for column: 'UnitPrice'\n","    raw_df = raw_df.withColumn('UnitPrice', raw_df['UnitPrice'].cast(T.DoubleType()))\n","    # Round column 'UnitPrice' (Number of decimals: 2)\n","    raw_df = raw_df.withColumn('UnitPrice', F.round(F.col('UnitPrice'), 2))\n","    return raw_df\n","\n","def clean_orders(raw_df):\n","    # Change column type to datetime64[ns] for columns: 'OrderDate', 'RequiredDate', 'ShippedDate'\n","    raw_df = raw_df.withColumn('OrderDate', raw_df['OrderDate'].cast(T.TimestampType()))\n","    raw_df = raw_df.withColumn('RequiredDate', raw_df['RequiredDate'].cast(T.TimestampType()))\n","    raw_df = raw_df.withColumn('ShippedDate', raw_df['ShippedDate'].cast(T.TimestampType()))\n","    # Rename column 'Freight' to 'FreightCosts'\n","    raw_df = raw_df.withColumnRenamed('Freight', 'FreightCosts')\n","    # Change column type to float64 for column: 'FreightCosts'\n","    raw_df = raw_df.withColumn('FreightCosts', raw_df['FreightCosts'].cast(T.DoubleType()))\n","    # Round column 'FreightCosts' (Number of decimals: 2)\n","    raw_df = raw_df.withColumn('FreightCosts', F.round(F.col('FreightCosts'), 2))\n","    # Capitalize the first character in column: 'ShipName'\n","    raw_df = raw_df.withColumn('ShipName', F.initcap(F.col('ShipName')))\n","    # Capitalize the first character in column: 'ShipAddress'\n","    raw_df = raw_df.withColumn('ShipAddress', F.initcap(F.col('ShipAddress')))\n","    return raw_df\n","\n","def clean_products(raw_df):\n","    # Capitalize the first character in column: 'ProductName'\n","    raw_df = raw_df.withColumn('ProductName', F.initcap(F.col('ProductName')))\n","    # Change column type to float64 for column: 'UnitPrice'\n","    raw_df = raw_df.withColumn('UnitPrice', raw_df['UnitPrice'].cast(T.DoubleType()))\n","    return raw_df\n","\n","def clean_region(raw_df):\n","    return raw_df\n","\n","def clean_shippers(raw_df):\n","    return raw_df\n","\n","def clean_suppliers(raw_df):\n","    # Capitalize the first character in column: 'Address'\n","    raw_df = raw_df.withColumn('Address', F.initcap(F.col('Address')))\n","    # Drop column: 'HomePage'\n","    raw_df = raw_df.drop('HomePage')\n","    return raw_df\n","\n","def clean_territories(raw_df):\n","    # Change column type to int64 for column: 'TerritoryID'\n","    raw_df = raw_df.withColumn('TerritoryID', raw_df['TerritoryID'].cast(T.LongType()))\n","    return raw_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"ea8d1e65-c08b-44cb-a810-194b6d1f5fe0","normalized_state":"finished","queued_time":"2026-01-05T23:28:31.182724Z","session_start_time":null,"execution_start_time":"2026-01-05T23:28:31.1838876Z","execution_finish_time":"2026-01-05T23:28:31.4494719Z","parent_msg_id":"14250d87-d90e-4c8c-87ca-726b327f71ab"},"text/plain":"StatementMeta(, ea8d1e65-c08b-44cb-a810-194b6d1f5fe0, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0f6d2f02-4cf9-4a38-a019-8ec5959a0dbc"},{"cell_type":"code","source":["cleaning_functions = {\n","    \"Categories\": clean_categories,\n","    \"Customers\": clean_customers,\n","    \"Employees\": clean_employees,\n","    \"EmployeeTerritories\": clean_employee_territories,\n","    \"OrderDetails\": clean_order_details,\n","    \"Orders\": clean_orders,\n","    \"Products\": clean_products,\n","    \"Region\": clean_region,\n","    \"Shippers\": clean_shippers,\n","    \"Suppliers\": clean_suppliers,\n","    \"Territories\": clean_territories\n","}\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"c60f3891-3d52-47e3-bfea-06ecbbdf6fa6","normalized_state":"finished","queued_time":"2026-01-05T21:39:35.0337528Z","session_start_time":null,"execution_start_time":"2026-01-05T21:39:35.0349591Z","execution_finish_time":"2026-01-05T21:39:35.2644178Z","parent_msg_id":"f3a36ddd-8058-49f6-836f-afc255871942"},"text/plain":"StatementMeta(, c60f3891-3d52-47e3-bfea-06ecbbdf6fa6, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dec44602-b25d-4edb-8a38-7c70d70549bc"},{"cell_type":"markdown","source":["#### **Data Loading function**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff19f0ea-b0d8-4ec8-9fd5-aa9623cb6e23"},{"cell_type":"code","source":["def upsert(dataframe: DataFrame, table_name: str, key_cols: list[str], partition_cols: list[str] = None, schema_name: str = None):\n","    \"\"\"\n","    Upsert into a Delta table in Microsoft Fabric Lakehouse.\n","\n","    If schema_name is provided, uses schema.table_name format and ensures the schema exists.\n","    \"\"\"\n","\n","    # Compose full name and path\n","    if schema_name:\n","        full_table_name = f\"{schema_name}.{table_name}\"\n","        table_path = f\"Tables/{schema_name}/{table_name}\"\n","        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n","    else:\n","        full_table_name = table_name\n","        table_path = f\"Tables/{table_name}\"\n","\n","    # Drop Duplicates\n","    dataframe = dataframe.dropDuplicates(key_cols)\n","\n","    if DeltaTable.isDeltaTable(spark, table_path):\n","        delta_table = DeltaTable.forPath(spark, table_path)\n","        merge_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in key_cols])\n","\n","        delta_table.alias(\"target\") \\\n","            .merge(\n","                source=dataframe.alias(\"source\"),\n","                condition=merge_condition\n","            ) \\\n","            .whenMatchedUpdateAll() \\\n","            .whenNotMatchedInsertAll() \\\n","            .execute()\n","\n","        print(f\"Upserted into existing table: {full_table_name}\")\n","    else:\n","        writer = dataframe.write.format(\"delta\").mode(\"overwrite\")\n","        if partition_cols:\n","            writer = writer.partitionBy(*partition_cols)\n","\n","        writer.saveAsTable(full_table_name)\n","        print(f\"Created new table: {full_table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"c60f3891-3d52-47e3-bfea-06ecbbdf6fa6","normalized_state":"finished","queued_time":"2026-01-05T21:45:11.8186085Z","session_start_time":null,"execution_start_time":"2026-01-05T21:45:11.8197028Z","execution_finish_time":"2026-01-05T21:45:12.188257Z","parent_msg_id":"2f68fff8-363c-4cc8-bbb2-8c3eb86f9409"},"text/plain":"StatementMeta(, c60f3891-3d52-47e3-bfea-06ecbbdf6fa6, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2afd89a7-78f6-4a2b-ab84-a8d07bdbf1ea"},{"cell_type":"markdown","source":["#### **Silver Ingestion**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ec7ea9fd-478e-43a0-a140-b4f9c9eaee07"},{"cell_type":"code","source":["for rec in records:\n","\n","    if rec.get(\"BronzeStatus\") != \"Success\":\n","        rec[\"SilverStatus\"] = \"Skipped\"\n","        continue\n","\n","    try:\n","        # Metadata\n","        DatabaseName = rec[\"DatabaseName\"].replace(\"_\", \"\").replace(\"-\", \"\")\n","        TableName = rec[\"TableName\"].strip().replace(\" \", \"\") #Example: Remove Space in \"Order Details\"\n","        TableSchema = rec[\"TableSchema\"]\n","        KeyColumn = rec[\"KeyColumn\"]\n","        DataDomain = str(rec[\"DataDomain\"]).lower()\n","\n","        # Read raw file\n","        df = spark.read.parquet(f\"Files/{rec['BronzeFolderPath']}\")\n","        df = df.dropDuplicates(rec[\"KeyColumn\"].split(\"|\")) \n","\n","        # Apply matching cleaning function\n","        cleaning_fn = cleaning_functions.get(TableName)\n","        if cleaning_fn:\n","            df = cleaning_fn(df)  # Apply it only if exists\n","        else:\n","            print(f\"⚠️ No cleaning function found for {TableName}, using raw DataFrame.\")\n","\n","        # Partition logic\n","        partition_cols = None\n","        if rec.get(\"PartitionColumn\"):\n","            part_col = rec[\"PartitionColumn\"]\n","            part_type = str(rec.get(\"PartitionType\", \"\")).strip().lower()\n","\n","            if part_type == \"date\":\n","                df = df.withColumn(part_col, F.to_date(F.col(part_col)))\n","                df = df.withColumn(\"Year\", F.year(F.col(part_col)))\n","                df = df.withColumn(\"Month\", F.month(F.col(part_col)))\n","                df = df.withColumn(\"Day\", F.dayofmonth(F.col(part_col)))\n","                partition_cols = [\"Year\", \"Month\", \"Day\"]\n","\n","            elif part_type == \"categorical\":\n","                partition_cols = [part_col]\n","\n","        # Upsert to Silver\n","        upsert(\n","            dataframe=df,\n","            table_name=TableName,\n","            key_cols=KeyColumn.split(\"|\"),\n","            partition_cols=partition_cols,\n","            schema_name=DataDomain \n","        )\n","\n","        rec[\"SilverStatus\"] = \"Success\"\n","        rec[\"SilverFolderPath\"] = f\"Tables/{DataDomain}/{TableName}\"\n","\n","    except Exception as e:\n","        rec[\"SilverStatus\"] = f\"Failed: {str(e)}\"\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e508c520-24d9-42da-a037-1a0dbf210935"},{"cell_type":"markdown","source":["#### **Notebook Return Output**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f106566-dd4e-4bd1-bdd4-54f0fa7824eb"},{"cell_type":"code","source":["# Optional: pretty-print the output\n","print(\"Returning payload →\", json.dumps(records, indent=2))\n","\n","# Return full JSON for pipeline/logging use\n","mssparkutils.notebook.exit(json.dumps(records, indent=2))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44f5f6b6-957b-4eba-a2b9-20b8645d526f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b6ffa431-4e32-4e55-af49-dc86806bb9ca"}],"default_lakehouse":"b6ffa431-4e32-4e55-af49-dc86806bb9ca","default_lakehouse_name":"NorthWind_LK","default_lakehouse_workspace_id":"f8cbb727-0c9a-4301-886c-e43a91759b60"}}},"nbformat":4,"nbformat_minor":5}